{
    "nbformat": 4, 
    "nbformat_minor": 0, 
    "cells": [
        {
            "source": "# The code was removed by DSX for sharing.", 
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "# Actions And Transformations\n\nRDDs support two types of operations: <b>transformations</b>, which create a new RDD from an existing one, and <b>actions</b>, which return a value to the driver program after running a computation on the dataset. \n\nAll transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset. This allows spark to optimize given transformation with a DAG structure.\n<br>\n\n<img src=\"http://image.prntscr.com/image/86fad7aa29dc4af1a85fae8a6a181eba.png\">", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "# Persistance\nBy default, each transformed RDD <b>may be</b> recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist (or cache) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes.\n<br>\n<img src=\"http://image.prntscr.com/image/71bdbbd1c3b34870ad0a0956d5e6a551.jpeg\">", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from pyspark.storagelevel import StorageLevel\nimport time", 
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }, 
        {
            "source": "filtered_rdd = sc.textFile(\"swift://CS340.\" + name + \"/ratings.csv\")\\\n                .map(lambda x: x.split(\",\"))\\\n                .map(lambda lis: lis[2])\\\n                .map(float)\\\n                .map(lambda x: x**2)\\\n                .map(int)\\\n                .filter(lambda x: x%2!=0)", 
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": []
        }, 
        {
            "source": "start =  time.time()\nfiltered_rdd.collect()\nprint time.time() - start", 
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "0.42010307312\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "start =  time.time()\nfiltered_rdd.collect()\nprint time.time() - start", 
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": [
                {
                    "name": "stdout", 
                    "text": "0.404487848282\n", 
                    "output_type": "stream"
                }
            ]
        }, 
        {
            "source": "Since they are not persisted explicitly, we would expect that both actions take the same amount of time. However that is not the case. There can be many reasons for this.\n\n1) Spark will automatically evict RDD partitions from Workers in an LRU(Least Recently Used) manner. The LRU eviction happens independently on each Worker and depends on the available memory in the Worker.(Chris Fregly)\n\n2) Spark also automatically persists some intermediate data in shuffle operations (e.g. reduceByKey), even without users calling persist.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# StorageLevel.MEMORY_ONLY \n<br> Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they're needed.\n\n# StorageLevel.MEMORY_AND_DISK\n<br> Almost same as MEMORY_ONLY, the only difference is it will store the partitions that don't fit the memory. So it does not have to recompute each time.\n# StorageLevel.MEMORY_ONLY_SER\n<br> Same as MEMORY_ONLY, the only difference is that it store RDD as <b>serialized</b> Java objects in the JVM.\n# StorageLevel.MEMORY_AND_DISK_SER\n<br> Combination on MEMORY_ONLY_SER and MEMORY_AND_DISK.\n# StorageLevel.DISK_ONLY\n<br> As name suggests its store the partitions to the disk.\n# MEMORY_ONLY_2, MEMORY_AND_DISK_2\n<br> Same as the levels above, but replicate each partition on two cluster nodes.\n\n<br>\n<b>Note</b>: In Python, stored objects will always be serialized with the <b>Pickle</b> library.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "filtered_rdd.persist(StorageLevel.DISK_ONLY)", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": []
        }, 
        {
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": []
        }
    ], 
    "metadata": {
        "language_info": {
            "nbconvert_exporter": "python", 
            "mimetype": "text/x-python", 
            "pygments_lexer": "ipython2", 
            "version": "2.7.11", 
            "file_extension": ".py", 
            "name": "python", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }
        }, 
        "kernelspec": {
            "name": "python2", 
            "display_name": "Python 2 with Spark 1.6", 
            "language": "python"
        }
    }
}