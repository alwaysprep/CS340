{
    "nbformat": 4, 
    "nbformat_minor": 0, 
    "cells": [
        {
            "source": "%matplotlib inline\nimport matplotlib.pyplot as plt", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "deletable": true, 
                "collapsed": false, 
                "editable": true
            }, 
            "outputs": []
        }, 
        {
            "source": "rdd = sc.parallelize([(1, \"a\"), (2, \"a\"), \n                      (3,\"a\"), (4, \"a\"), \n                      (5, \"a\"), (6, \"a\"),\n                      (1,\"a\"), (7,\"a\"), \n                      (7,\"a\"), (9,\"a\"), (21, \"a\")], 4)\n\nrdd2 = rdd.mapValues(lambda x: \"b\")\nprint rdd.getNumPartitions()\nprint rdd.partitioner  # has no partitioner\nprint sc.defaultParallelism", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": []
        }, 
        {
            "source": "rdd.glom().collect()", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": []
        }, 
        {
            "source": "rdd2.glom().collect()", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": []
        }, 
        {
            "source": "rdd.union(rdd2).getNumPartitions()", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": []
        }, 
        {
            "source": "rdd.union(rdd2).glom().collect()", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": []
        }, 
        {
            "source": "partitioned_rdd = rdd.partitionBy(20)", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": []
        }, 
        {
            "source": "partitioned_rdd.partitioner.partitionFunc # has hashing partitioner", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": []
        }, 
        {
            "source": "rdd.map(lambda (x,y):(y,x)).partitionBy(20).glom().collect()", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": []
        }, 
        {
            "source": "print(hash(\"a\") % 20) #\u00a0spark knows how to shuffle the data by hashing the key", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": []
        }, 
        {
            "source": "rdd.partitionBy(4).glom().collect()", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": []
        }, 
        {
            "source": "rdd2.partitionBy(4).glom().collect()", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": []
        }, 
        {
            "source": "rdd = rdd.partitionBy(4)\nrdd2 = rdd2.partitionBy(4)\nrdd.join(rdd2).collect()", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": []
        }, 
        {
            "source": "rdd.join(rdd2).glom().collect()", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": []
        }, 
        {
            "source": "rdd = rdd.partitionBy(5)\nrdd2 = rdd2.partitionBy(7)\nrdd.join(rdd2).glom().collect()", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": []
        }, 
        {
            "source": "# mapPartitions\n\nThe first parameter is going to be a partitition of a RDD as iterator. While in the map scenario, the first parameter is an item in the RDD.\n\n#### rdd.map(lambda item: f(item))              $\\hspace{27 mm}$                --> item\n####\u00a0rdd.mapPartitions(lambda iterator: f(iterator)) --> iterator\n<br>", 
            "cell_type": "markdown", 
            "metadata": {
                "deletable": true, 
                "editable": true
            }
        }, 
        {
            "source": "rdd.glom().collect()", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": false
            }, 
            "outputs": []
        }, 
        {
            "source": "def func(partition):\n    return (list(partition),)\n\nrdd.mapPartitions(func).collect()", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "deletable": true, 
                "collapsed": false, 
                "editable": true
            }, 
            "outputs": []
        }, 
        {
            "source": "def func(partition):\n    return [len(list(partition))]\n\nnum_of_elements = rdd.mapPartitions(func).collect()\n\nprint(num_of_elements)\nplt.bar(range(len(num_of_elements)), num_of_elements, width=0.8, color=\"blue\")", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "deletable": true, 
                "collapsed": false, 
                "editable": true
            }, 
            "outputs": []
        }, 
        {
            "source": "from pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\n# data file included inside the folder\nfil = sc.textFile(\"Path to data.txt file\")\\\n        .flatMap(lambda line: [(word,1) for word in line.split()])\n\nfil = fil.partitionBy(200)\n\nnum_of_elements = fil.mapPartitions(func).collect()\n\nplt.bar(range(len(num_of_elements)), num_of_elements, width=0.8, color=\"blue\")", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "deletable": true, 
                "collapsed": false, 
                "editable": true
            }, 
            "outputs": []
        }, 
        {
            "source": "#\u00a0mapPartitionWithIndex", 
            "cell_type": "markdown", 
            "metadata": {
                "deletable": true, 
                "editable": true
            }
        }, 
        {
            "source": "The first parameter is going to be index of the partitition and the second parameter is partition itself.\n\n####\u00a0rdd.mapPartitions(lambda iterator: f(iterator)) $\\hspace{28 mm}$  --> iterator\n#### rdd.mapPartitionsWithIndex(lambda index, iterator: f(iterator)) --> iterator\n", 
            "cell_type": "markdown", 
            "metadata": {
                "deletable": true, 
                "editable": true
            }
        }, 
        {
            "source": "# sampling data by dumping partitions (not recommended)\ndef func(index, partition, percent):\n    mod = 100/percent\n    if index%mod == 0:\n        return partition\n    else:\n        return []\n    \n# sampling items inside the partitions (recommended)\ndef func(index, partition, percent):\n    mod = 100 / percent\n    lis = []\n    for idx, item in enumerate(partition):\n        if idx % mod == 0:\n            lis.append(item)\n    return lis\n    \nprint fil.count()\nprint fil.mapPartitionsWithIndex(lambda index, partition: func(index, partition, 10)).count()", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "deletable": true, 
                "collapsed": false, 
                "editable": true
            }, 
            "outputs": []
        }, 
        {
            "source": "# foreachPartition", 
            "cell_type": "markdown", 
            "metadata": {
                "deletable": true, 
                "editable": true
            }
        }, 
        {
            "source": "This methos does something with rdd, but does not return an RDD back. Generally used to write data to databases.", 
            "cell_type": "markdown", 
            "metadata": {
                "deletable": true, 
                "editable": true
            }
        }, 
        {
            "source": "import MySQLdb\n\ndb = MySQLdb.connect(\"ip_address\",\"user_name\",\"password\",\"database_name\" )\ncursor = db.cursor()\n\ncursor.execute('INSERT INTO TESTDB.letter_number VALUES (\"%s\", %d)' % (\"a\",123))\n\ndb.commit()", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "deletable": true, 
                "collapsed": false, 
                "editable": true
            }, 
            "outputs": []
        }, 
        {
            "source": "def func(partition):\n    db = MySQLdb.connect(\"ip_address\",\"user_name\",\"password\",\"database_name\" )\n    cursor = db.cursor()\n    for item in partition:\n        cursor.execute('INSERT INTO TESTDB.letter_number VALUES (\"%s\", %d)' % (item[0], item[1]))\n    db.commit()\n        \nrdd.foreachPartition(func)", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "deletable": true, 
                "collapsed": false, 
                "editable": true
            }, 
            "outputs": []
        }, 
        {
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "deletable": true, 
                "collapsed": false, 
                "editable": true
            }, 
            "outputs": []
        }, 
        {
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "deletable": true, 
                "collapsed": false, 
                "editable": true
            }, 
            "outputs": []
        }, 
        {
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "deletable": true, 
                "collapsed": false, 
                "editable": true
            }, 
            "outputs": []
        }, 
        {
            "source": "", 
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "deletable": true, 
                "collapsed": true, 
                "editable": true
            }, 
            "outputs": []
        }
    ], 
    "metadata": {
        "language_info": {
            "nbconvert_exporter": "python", 
            "mimetype": "text/x-python", 
            "pygments_lexer": "ipython2", 
            "version": "2.7.11", 
            "file_extension": ".py", 
            "name": "python", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }
        }, 
        "kernelspec": {
            "name": "python2", 
            "display_name": "Python 2 with Spark 1.6", 
            "language": "python"
        }
    }
}